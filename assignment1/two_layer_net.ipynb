{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03747fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Tools.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d97682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.7698500479884e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = layer_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026da70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  1.0908199508708189e-10\n",
      "dw error:  2.1752635504596857e-10\n",
      "db error:  7.736978834487815e-12\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "from Tools.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: layer_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: layer_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: layer_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = layer_forward(x, w, b)\n",
    "dx, dw, db = layer_backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328d6bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.999999798022158e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7249759d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.2756349136310288e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea0a415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing layer_relu_forward and layer_relu_backward:\n",
      "dx error:  6.395535042049294e-11\n",
      "dw error:  8.162011105764925e-11\n",
      "db error:  7.826724021458994e-12\n"
     ]
    }
   ],
   "source": [
    "# test layer & relu\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = layer_relu_forward(x, w, b)\n",
    "dx, dw, db = layer_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: layer_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: layer_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: layer_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# Relative error should be around e-10 or less\n",
    "print('Testing layer_relu_forward and layer_relu_backward:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88531583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing svm_loss:\n",
      "loss:  8.999602749096233\n",
      "dx error:  1.4021566006651672e-09\n",
      "\n",
      "Testing softmax_loss:\n",
      "loss:  2.302545844500738\n",
      "dx error:  9.483503037636722e-09\n",
      "[ 0.00199856  0.00199983  0.00199757  0.00200115  0.0020019  -0.01799971\n",
      "  0.00199895  0.00200102  0.00199949  0.00200123]\n",
      "[ 0.00199856  0.00199983  0.00199757  0.00200115  0.0020019  -0.01799971\n",
      "  0.00199895  0.00200102  0.00199949  0.00200123]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "\n",
    "# Test svm_loss function. Loss should be around 9 and dx error should be around the order of e-9\n",
    "print('Testing svm_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print(dx_num[1,:])\n",
    "print(dx[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18603dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization)\n",
      "Running numeric gradient check with reg =  0.0\n",
      "W1 relative error: 1.22e-08\n",
      "W2 relative error: 3.42e-10\n",
      "b1 relative error: 6.55e-09\n",
      "b2 relative error: 2.53e-10\n",
      "Running numeric gradient check with reg =  0.7\n",
      "W1 relative error: 2.53e-07\n",
      "W2 relative error: 1.37e-07\n",
      "b1 relative error: 1.56e-08\n",
      "b2 relative error: 9.09e-10\n"
     ]
    }
   ],
   "source": [
    "from Tools.fc_net import *\n",
    "np.random.seed(231)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "# Errors should be around e-7 or less\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ce84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear previously loaded data\n",
      "Training data shape:  (50000, 3072)\n",
      "Training labels shape:  (50000,)\n",
      "Test data shape:  (10000, 3072)\n",
      "Test labels shape:  (10000,)\n",
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000,)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from Tools.data_utils import load_CIFAR10\n",
    "\n",
    "cifar10_dir = 'C:/Users/rainstar/Jupyter Folder/cs231n/cifar-10-batches-py/'\n",
    "\n",
    "try:\n",
    "    del X_train,Y_train\n",
    "    del X_test,Y_test\n",
    "    print('Clear previously loaded data')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "X_train,Y_train,X_test,Y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "print('Training data shape: ',X_train.shape)\n",
    "print('Training labels shape: ',Y_train.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test labels shape: ',Y_test.shape)\n",
    "\n",
    "num_train = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev =  500\n",
    "\n",
    "mask = range(num_train,num_train+num_validation)\n",
    "X_val = X_train[mask]\n",
    "Y_val = Y_train[mask].astype('int')\n",
    "\n",
    "mask = range(num_train)\n",
    "X_train = X_train[mask]\n",
    "Y_train = Y_train[mask].astype('int')\n",
    "\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "Y_test = Y_test[mask].astype('int')\n",
    "\n",
    "mask = np.random.choice(num_train,num_dev,replace=True)\n",
    "X_dev = X_train[mask]\n",
    "Y_dev = Y_train[mask].astype('int')\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', Y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', Y_val.shape)\n",
    "print('Test data shape: ', Y_test.shape)\n",
    "print('Test labels shape: ', Y_test.shape)\n",
    "\n",
    "# normalize\n",
    "mean_image = np.mean(X_train,axis=0)\n",
    "\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n",
    "# append bias\n",
    "# X_train = np.hstack([X_train,np.ones((X_train.shape[0],1))])\n",
    "# X_val = np.hstack([X_val,np.ones((X_val.shape[0],1))])\n",
    "# X_test = np.hstack([X_test,np.ones((X_test.shape[0],1))])\n",
    "# X_dev = np.hstack([X_dev,np.ones((X_dev.shape[0],1))])\n",
    "\n",
    "# print('X_train: ',X_train.shape)\n",
    "# print('X_val: ',X_val.shape)\n",
    "# print('X_test: ',X_test.shape)\n",
    "# print('X_dev: ',X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568031fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 4900) loss: 2.304261\n",
      "(Epoch 0 / 10) train acc: 0.136000; val_acc: 0.149000\n",
      "(Iteration 101 / 4900) loss: 1.783493\n",
      "(Iteration 201 / 4900) loss: 1.663331\n",
      "(Iteration 301 / 4900) loss: 1.801142\n",
      "(Iteration 401 / 4900) loss: 1.574793\n",
      "(Epoch 1 / 10) train acc: 0.440000; val_acc: 0.444000\n",
      "(Iteration 501 / 4900) loss: 1.516751\n",
      "(Iteration 601 / 4900) loss: 1.570565\n",
      "(Iteration 701 / 4900) loss: 1.492328\n",
      "(Iteration 801 / 4900) loss: 1.430815\n",
      "(Iteration 901 / 4900) loss: 1.369015\n",
      "(Epoch 2 / 10) train acc: 0.495000; val_acc: 0.466000\n",
      "(Iteration 1001 / 4900) loss: 1.525226\n",
      "(Iteration 1101 / 4900) loss: 1.489596\n",
      "(Iteration 1201 / 4900) loss: 1.272572\n",
      "(Iteration 1301 / 4900) loss: 1.424399\n",
      "(Iteration 1401 / 4900) loss: 1.487955\n",
      "(Epoch 3 / 10) train acc: 0.506000; val_acc: 0.479000\n",
      "(Iteration 1501 / 4900) loss: 1.369955\n",
      "(Iteration 1601 / 4900) loss: 1.203881\n",
      "(Iteration 1701 / 4900) loss: 1.323502\n",
      "(Iteration 1801 / 4900) loss: 1.297813\n",
      "(Iteration 1901 / 4900) loss: 1.331007\n",
      "(Epoch 4 / 10) train acc: 0.558000; val_acc: 0.489000\n",
      "(Iteration 2001 / 4900) loss: 1.390419\n",
      "(Iteration 2101 / 4900) loss: 1.467728\n",
      "(Iteration 2201 / 4900) loss: 1.285327\n",
      "(Iteration 2301 / 4900) loss: 1.247591\n",
      "(Iteration 2401 / 4900) loss: 1.420237\n",
      "(Epoch 5 / 10) train acc: 0.526000; val_acc: 0.485000\n",
      "(Iteration 2501 / 4900) loss: 1.414070\n",
      "(Iteration 2601 / 4900) loss: 1.342017\n",
      "(Iteration 2701 / 4900) loss: 1.362017\n",
      "(Iteration 2801 / 4900) loss: 1.490308\n",
      "(Iteration 2901 / 4900) loss: 1.275680\n",
      "(Epoch 6 / 10) train acc: 0.526000; val_acc: 0.481000\n",
      "(Iteration 3001 / 4900) loss: 1.244257\n",
      "(Iteration 3101 / 4900) loss: 1.536911\n",
      "(Iteration 3201 / 4900) loss: 1.314755\n",
      "(Iteration 3301 / 4900) loss: 1.486513\n",
      "(Iteration 3401 / 4900) loss: 1.543751\n",
      "(Epoch 7 / 10) train acc: 0.544000; val_acc: 0.473000\n",
      "(Iteration 3501 / 4900) loss: 1.237873\n",
      "(Iteration 3601 / 4900) loss: 1.248757\n",
      "(Iteration 3701 / 4900) loss: 1.128763\n",
      "(Iteration 3801 / 4900) loss: 1.245108\n",
      "(Iteration 3901 / 4900) loss: 1.451216\n",
      "(Epoch 8 / 10) train acc: 0.541000; val_acc: 0.486000\n",
      "(Iteration 4001 / 4900) loss: 1.062496\n",
      "(Iteration 4101 / 4900) loss: 1.396724\n",
      "(Iteration 4201 / 4900) loss: 1.231986\n",
      "(Iteration 4301 / 4900) loss: 1.033305\n",
      "(Iteration 4401 / 4900) loss: 1.412426\n",
      "(Epoch 9 / 10) train acc: 0.578000; val_acc: 0.470000\n",
      "(Iteration 4501 / 4900) loss: 1.093631\n",
      "(Iteration 4601 / 4900) loss: 1.334526\n",
      "(Iteration 4701 / 4900) loss: 1.195366\n",
      "(Iteration 4801 / 4900) loss: 1.125483\n",
      "(Epoch 10 / 10) train acc: 0.561000; val_acc: 0.482000\n"
     ]
    }
   ],
   "source": [
    "from Tools.solver import *\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves about 36% #\n",
    "# accuracy on the validation set.                                            #\n",
    "##############################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "data = {}\n",
    "data['X_train'] = X_train\n",
    "data['X_val'] = X_val\n",
    "data['y_train'] = Y_train\n",
    "data['y_val'] = Y_val\n",
    "solver = Solver(model, data,\n",
    "                    update_rule='sgd',\n",
    "                    optim_config={\n",
    "                      'learning_rate': 1e-3,\n",
    "                    },\n",
    "                    lr_decay=0.95,\n",
    "                    num_epochs=10, batch_size=100,\n",
    "                    print_every=100)\n",
    "solver.train()\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf5a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
